{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e118c05f-60bb-417e-a35c-d1edc2095dc1",
   "metadata": {},
   "source": [
    "# **Connectionist Temporal Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a75f9-942e-4269-9a4b-c328ea4013f9",
   "metadata": {},
   "source": [
    "O CTC (do inglês, *Connectionist Temporal Classification*) é algoritmo utilizado para treinar modelos profundos de redes neurais para tarefas como reconhecimento de fala e problemas de sequência. Em problemas de reconhecimento de fala um dos maiores problemas é entender quantas amostras do áudio que devem ser mapeadas para um certo caracter ou fonema, isto é, não existe alinhamento entre a sequência de entrada e a sequência de saída."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86471ce-9e41-47f7-8e5f-8d2039e66d35",
   "metadata": {},
   "source": [
    "## **O problema**\n",
    "Seja $X$, um áudio, isto é, $X = [x_1, x_2, ..., x_t]$ e $Y$ uma transcrição, isto é $Y = [y_1, y_2, ..., y_v]$. A nossa tarefa é encontrar $f$ tal que:\n",
    "\n",
    "$$\n",
    "f: X \\rightarrow Y\n",
    "$$\n",
    "\n",
    "Tanto os elementos de $X$ quanto $Y$ podem variar de tamanho. Impossibilitando técnicas clássicas de aprendizado de máquina. O CTC contorna esse problema ao retornar uma distribuição de probabilidade de **Y** (de todo possível valor) para cada **X**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818b3d6-3a64-4842-930c-06e69bdb2f83",
   "metadata": {},
   "source": [
    "Exemplo:\n",
    "\n",
    "Uma entrada de tamanho 5 e $Y = [a,n,a]$ (dois possiveis tokens):\n",
    "\n",
    "- Primeiro associamos cada $x$ ao elemento mais provável de $Y$.\n",
    "- Colapsa os elementos repetidos.\n",
    "\n",
    "| entrada:        |  x1 |  x2 |  x3 |  x4 |  x5 |\n",
    "| :-              | :-: | :-: | :-: | :-: | :-: |\n",
    "|**alinhamento**: | a   | n   | n   | a   | a   |\n",
    "|**Saida**:       | a   | n   |     | a   |     |\n",
    "\n",
    "Um problema com essa metodologia:\n",
    "\n",
    "- Como podemos escrever palavras como Cachorro ou Osso ? Uma vez que quando colapsado teríamos Cachoro ou Oso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ccac5-1396-454b-b76b-3630757e249d",
   "metadata": {},
   "source": [
    "Para resolver esse problema o algoritmo do CTC introduz um *token* $\\epsilon$ normalmente denominado *blank token*. <font color =\"red\"> Esse Token não tem nenhuma correspondência (Ele é diferente do token espaço em branco)</font>.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "Uma entrada de tamanho 7 e $Y = [o,s,s,o]$ (três possiveis tokens - {o,s,$\\epsilon$}):\n",
    "\n",
    "| entrada:        | x1 |  x2 |  x3 |  x4 |  x5 | x6 | x7 |\n",
    "| :-              | :-: | :-: | :-: | :-: | :-: |:-: | :-: |\n",
    "|**alinhamento**: | o   | s   | $\\epsilon$| s  | s | o  | o | o |\n",
    "|**colapsa**:     | o   | s   | $\\epsilon$| s  |   | o  |  |  |\n",
    "|**remove** $\\epsilon$:     | o   | s   | | s  |   | o  |  |  |\n",
    "\n",
    "\n",
    "Existem vários alinhamentos possíveis, ainda no mesmo exemplo, algumas opções <font color=\"orange\">**válidas**</font> de tamanho 7 são:\n",
    "\n",
    "- [o, $\\epsilon$, s, $\\epsilon$, s, $\\epsilon$, o] $\\rightarrow$ [o, $\\epsilon$, s, $\\epsilon$, s, $\\epsilon$, o] $\\rightarrow$ [o, s, s, o]\n",
    "- [$\\epsilon$, o, s, $\\epsilon$, s, $\\epsilon$, o] $\\rightarrow$ [$\\epsilon$, o, s, $\\epsilon$, s, $\\epsilon$, o] $\\rightarrow$ [o, s, s, o]\n",
    "- [$\\epsilon$, o, s, $\\epsilon$, s, o, $\\epsilon$] $\\rightarrow$ [$\\epsilon$, o, s, $\\epsilon$, s, o, $\\epsilon$] $\\rightarrow$ [o, s, s, o]\n",
    "- [o, o, s, $\\epsilon$, s, o, $\\epsilon$] $\\rightarrow$ [o, s, $\\epsilon$, s, o, $\\epsilon$] $\\rightarrow$ [o, s, s, o]\n",
    "\n",
    "O alinhamento de $X$ e $Y$ funciona da forma *many-to-one*, isto é, diferentes alinhamentos levam ao mesmo $Y$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947d529-e1b9-49a8-872c-d0e5a55849c9",
   "metadata": {},
   "source": [
    "# **Decoding** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e810e495-e34a-4429-ba8c-e829ec0e139b",
   "metadata": {},
   "source": [
    "Vamos supor que $Y = [h, e, l, l, o]$, com tokens $\\mathcal{Y} = \\{h,e,l,o, \\epsilon\\}$. A cada *time-step* temos como saída uma distribuição de probabilidade de todos os tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0723ffa9-0c30-464b-a554-37c1cc91aa22",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/ctc_condional_proba.png\" width=400/></center>\n",
    "\n",
    "Ref: https://distill.pub/2017/ctc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa3cb8-d592-4ded-b683-e3fe678f1151",
   "metadata": {},
   "source": [
    "Agora podemos calcular as probabilidade de uma dada palavra (sequência) acontecer:\n",
    "\n",
    "- Multiplicar as probabilidade de cada instante $t$ para os diferentes possíveis caminhos.  \n",
    "- Somar as probabilidades dos caminhso que levam a mesma palavra (sequência).\n",
    "\n",
    "\n",
    "Seja $Y = [a]$, os tokens $\\mathcal{Y} = \\{a, \\epsilon \\}$ e $X$ de tamanho $2$. Temos:\n",
    "\n",
    "$P(Y|X) = \\sum_{A \\in A_{X,Y}} \\prod p_t(v_t|X) = p_{a,t_1}p_{\\epsilon,t_2} + p_{\\epsilon,t_1}p_{a,t_2} + p_{a,t_1}p_{a,t_2}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf05710-95ee-4f60-87f9-65f825459f7f",
   "metadata": {},
   "source": [
    "# **Calculando a loss** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac63720-929c-4e5a-8068-8772b066744d",
   "metadata": {},
   "source": [
    "Utilizando programação dinâmica podemos fazer uma busca nesse grafo de forma mais eficiente. Um exemplo visual desse processo para $Y = [a, b]$, os tokens $\\mathcal{Y} = \\{a, b,\\epsilon \\}$ e $X$ de tamanho $6$.\n",
    "<center><img src=\"figures/example_decoding.png\" width=400/></center>\n",
    "\n",
    "Ref: https://distill.pub/2017/ctc/\n",
    "\n",
    "Então o nosso modelo retorna uma distribuição de probabilidade para cada instante de tempo $t$. Utiliza-se essa distribuição para estimar a probabilidade da sequência. Tudo isso com o objetivo de calcular a *loss*. Seja $\\mathcal{D}$ o conjunto de treino os parâmetros do modelo são tunados para minimizar a seguinte equação:\n",
    "\n",
    "$$\\mathcal{L}(X,Y) = \\sum_{(X,Y \\in \\mathcal{D})} -\\log p(Y|X)$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeadc99-4d05-476d-837d-a54cdd27664d",
   "metadata": {},
   "source": [
    "# **Inferência** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b788856-50e1-4f6b-8681-f7adc26cc1f2",
   "metadata": {},
   "source": [
    "Nesse passo temos um modelo treinado que retorna a melhor distribuição de probabilidades possível, então basta resolver:\n",
    "\n",
    "$$Y^* = \\mathop{\\arg \\max}\\limits_{Y} p(Y|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681a526-0d90-4243-a9c2-a4f9058e8288",
   "metadata": {},
   "source": [
    "Agora precisamos calcular p(Y|X) para todas as possiveis sequências e pegar a melhor? Uma possível técnica é pegar o token de maior probabilidade a cada *step*, técnica conhecida como ***best path decoding***. Esse processo é conhecido como decoding da sequência. Existem várias técnicas de decoding como exemplo, ***beam-search decoding***, ***prefix-search decoding*** e ***token passing***.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146e773-6142-4fba-b14e-b648f8ae0e76",
   "metadata": {},
   "source": [
    "**Referencias**\n",
    "\n",
    "- https://distill.pub/2017/ctc/\n",
    "- https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
