{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf4fb07-6c28-40cd-8921-6d03db78ec90",
   "metadata": {},
   "source": [
    "https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b07a4e9-3f28-43b8-be57-df59601af589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d469725c-63ed-4b50-a84d-7d506ddb292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['hey how are you', 'good i am fine', 'have a nice day']\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8b56e-aa79-467d-8d24-b4a836bc126d",
   "metadata": {},
   "source": [
    "Next, we'll be padding our input sentences to ensure that all the sentences are of standard length. While RNNs are typically able to take in variably sized inputs, we will usually want to feed training data in batches to speed up the training process. In order to used batches to train on our data, we'll need to ensure that each sequence within the input data is of equal size.\n",
    "\n",
    "Therefore, in most cases, padding can be done by filling up sequences that are too short with 0 values and trimming sequences that are too long. In our case, we'll be finding the length of the longest sequence and padding the rest of the sentences with blank spaces to match that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e8ab66-bac2-4c31-ba50-5c9be58a2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the length of the longest string in our data\n",
    "maxlen = len(max(text, key=len))\n",
    "\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of\n",
    "# the sentence matches the length of the longest sentence\n",
    "for i in range(len(text)):\n",
    "    while len(text[i])<maxlen:\n",
    "        text[i] += ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232be064-1af5-4e5e-a475-a5ca94c9b868",
   "metadata": {},
   "source": [
    "\n",
    "- Input data\n",
    "    - The last input character should be excluded as it does not need to be fed into the model\n",
    "- Target/Ground Truth Label\n",
    "    - One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ba6efc-58c2-4fa7-bfca-9632170c3f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # Remove first character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8cb5414-5471-4a7d-b80f-d38819571919",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66975e7-7af6-4b63-86bd-753227befff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 5, 7, 16, 3, 12, 0, 16, 9, 8, 5, 16, 7, 12],\n",
       " [1, 12, 12, 13, 16, 4, 16, 9, 10, 16, 14, 4, 2, 5],\n",
       " [3, 9, 6, 5, 16, 9, 16, 2, 4, 11, 5, 16, 13, 9]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "201bd811-18f5-49bf-a592-f60333205126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 7, 16, 3, 12, 0, 16, 9, 8, 5, 16, 7, 12, 15],\n",
       " [12, 12, 13, 16, 4, 16, 9, 10, 16, 14, 4, 2, 5, 16],\n",
       " [9, 6, 5, 16, 9, 16, 2, 4, 11, 5, 16, 13, 9, 7]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d966d66-49fb-4610-a8f8-5354e634da80",
   "metadata": {},
   "source": [
    "Before encoding our input sequence into one-hot vectors, we'll define 3 key variables:\n",
    "\n",
    "- **dict_size:** Dictionary size - The number of unique characters that we have in our text. This will determine the one-hot vector size as each character will have an assigned index in that vector\n",
    "- **seq_len:** The length of the sequences that we're feeding into the model. As we standardized the length of all our sentences to be equal to the longest sentences, this value will be the max length - 1 as we removed the last character input as well\n",
    "    \n",
    "- **batch_size:** The number of sentences that we defined and are going to feed into the model as a batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82fc14f-14ed-49e8-81ba-d7ee215f1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe63fe9-3ef7-462d-a47e-72fa9ae5a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225f4b83-48a2-44bb-b4d6-913dee9e3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8325939-b5e0-4a62-a4ba-7856f17bd2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "111a9951-a747-40c9-b078-6e68d11f0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ada0250-916c-4b8b-a2fc-f18bfbcb2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08f8a6ac-0191-42a2-a006-59598dd27c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 2.3817\n",
      "Epoch: 20/100............. Loss: 2.0432\n",
      "Epoch: 30/100............. Loss: 1.6704\n",
      "Epoch: 40/100............. Loss: 1.3119\n",
      "Epoch: 50/100............. Loss: 0.9768\n",
      "Epoch: 60/100............. Loss: 0.7082\n",
      "Epoch: 70/100............. Loss: 0.5075\n",
      "Epoch: 80/100............. Loss: 0.3599\n",
      "Epoch: 90/100............. Loss: 0.2626\n",
      "Epoch: 100/100............. Loss: 0.1992\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd627ab-a246-421b-a9d6-032f4f7e3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "600b84a0-bb1b-4560-9299-c50cec5aca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81900d27-7186-4734-831b-d046cceaf5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good i am fine '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 15, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89630c2e-b418-4afe-a91b-2ff8853842f0",
   "metadata": {},
   "source": [
    "Although it may look as if each RNN cell is using a different weight as shown in the graphics, all of the weights are actually the same as that RNN cell is essentially being re-used throughout the process. Therefore, only the input data and hidden state carried forward are unique at each time step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
