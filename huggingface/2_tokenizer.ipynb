{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7445f666-d3f2-4c7e-9d70-809eb29f4c14",
   "metadata": {},
   "source": [
    "# **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0fd2c-0329-4ddf-a3d6-522b28ff4025",
   "metadata": {},
   "source": [
    "## **TOC:**\n",
    "\n",
    "- 1) **[Introduction](#intro)**\n",
    "\n",
    "- 2) **[Character Tokenization](#chartoken)**\n",
    "\n",
    "- 3) **[Word Tokenization](#wordtoken)**\n",
    "\n",
    "- 4) **[Subword Tokenization](#subwordtoken)**\n",
    "\n",
    "    - 4.1) **[Auto Tokenizer](#autotokenizer)**\n",
    "    - 4.2) **[Specific Tokenizer](#specifictokenizer)**\n",
    "\n",
    "- 5) **[Tokenizing the Dataset](#tokenizingdataset)**\n",
    "    \n",
    "    - 5.1) **[HuggingFace Dataset](#huggingdataset)**\n",
    "    \n",
    "    - 5.2) **[Custom Dataset](#customdataset)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2529a1-48ed-4e23-b860-89282dc0f7d4",
   "metadata": {},
   "source": [
    "Wrapper de um dicionario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ea5e3f-709c-4be7-a689-76d59b974bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset emotion (/home/rocabrera/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabb482cc139477698f4aba1d15bf9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# The base class Dataset implements a Dataset backed by an Apache Arrow table.\n",
    "emotions = load_dataset(\"emotion\") ; emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a436ed-0914-4d21-86fe-05e7e1d60488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = emotions[\"train\"] ; train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9856d7-74f0-4d0a-aedc-6687436d6293",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08522788-c58d-40a5-be50-50a9c940bd69",
   "metadata": {},
   "source": [
    "## 2) **Character Tokenization** <a class=\"anchor\" id=\"chartoken\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0eab3-b4a9-4fbc-955b-2ad435346d94",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- https://huggingface.co/docs/datasets/process\n",
    "- https://huggingface.co/docs/datasets/v2.3.2/en/package_reference/main_classes#datasets.Dataset.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330c305e-ce1e-4130-8160-cd9007c2f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming everything fits in memory\n",
    "vocab = set(\"\".join(train_ds[\"text\"]))\n",
    "char_mapping = {ch: idx for idx, ch in enumerate(sorted(vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c5a40cb-589d-4ebb-9d05-acc9e1e31d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a536a7f2b24743b9b3543c03777def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "def char_non_batched_tokenizer(batch, mapping):\n",
    "\n",
    "    mapped_tokens = [mapping[char] for char in batch[\"text\"]]\n",
    "        \n",
    "    return {\"input_ids\": mapped_tokens}\n",
    "\n",
    "# function(example: Dict[str, Any]) -> Dict[str, Any]\n",
    "_ = train_ds.map(lambda x: char_non_batched_tokenizer(x, char_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7541b30-a619-4d57-9c6b-41f08d275ac0",
   "metadata": {},
   "source": [
    "The default batch size is 1000, but you can adjust it with the ```batch_size``` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f310e9a-23e0-4dda-bb8c-ef9f9f3af3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30446fce07e347a387bcb5b475af9303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "def char_batched_tokenizer(batch, mapping):\n",
    "    \n",
    "    mapped_tokens = [[mapping[char] for char in list(sentence)] for sentence in batch[\"text\"]]\n",
    "    \n",
    "    return {\"input_ids\": mapped_tokens}    \n",
    "\n",
    "_ = train_ds.map(lambda x: char_batched_tokenizer(x, char_mapping), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779664ae-2e47-4523-9d03-760e7c9a0c14",
   "metadata": {},
   "source": [
    "Set the ```num_proc``` argument to set the number of processes to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d826bef7-1014-48af-87e4-876ada0536fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9de80e2363456ca0c5ee905f6e9469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b727d19a964dc38c82ed3286d7cf98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c430fdd4364ab1985a405636decb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e340fd2968a940f5b6dbca2770964912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "def char_tokenizer(batch, mapping):\n",
    "    \n",
    "    if isinstance(batch[\"text\"], list):\n",
    "        mapped_tokens = [[mapping[char] for char in list(sentence)] for sentence in batch[\"text\"]]\n",
    "    else:\n",
    "        mapped_tokens = [mapping[char] for char in batch[\"text\"]]\n",
    "        \n",
    "    return {\"input_ids\": mapped_tokens}\n",
    "\n",
    "_ = train_ds.map(lambda x: char_tokenizer(x, char_mapping), batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15298a-4fdd-4e21-9a4d-66a9ef79a428",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856012e9-a6ed-48aa-96c8-978079572d3b",
   "metadata": {},
   "source": [
    "## 3) **Word Tokenization** <a class=\"anchor\" id=\"wordtoken\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e945edc-8214-43c4-b156-c79a0371b2c6",
   "metadata": {},
   "source": [
    "Using word tokenization enables the model to skip the step of\n",
    "learning words from characters, and thereby reduces the complexity of the training\n",
    "process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e0b7d40-cd05-4f68-9ab5-d61464bf3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming everything fits in memory\n",
    "vocab = set(\"\".join(train_ds[\"text\"]).split())\n",
    "word_mapping = {word: idx for idx, word in enumerate(sorted(vocab))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc3e564-3e09-4998-a481-3321bcc2b0e6",
   "metadata": {},
   "source": [
    "**Problems:**\n",
    "- Punctuation is not accounted.\n",
    "- Declinations, conjugations and misspellings are not accounted.\n",
    "- The size of the vocabulary can easily grow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56149769-6c79-4302-9b1e-89136bc0ca71",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe136cc-3f8a-411b-bc0d-a5da3bf49d57",
   "metadata": {},
   "source": [
    "## 3) **Subword Tokenization** <a class=\"anchor\" id=\"subwordtoken\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86fb310-bf4e-4b0a-b93b-51215ac01146",
   "metadata": {},
   "source": [
    "Transformers provides a convenient AutoTokenizer class that allows you to quickly load the tokenizer associated with a pretrained model. There are several subword tokenization algorithms, such as **Byte-Pair Encoding** and **WordPiece**. More information can be found [here](https://huggingface.co/course/chapter6/1?fw=pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffeeb409-1a54-4d0c-9a53-dccf9659f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f863ba7-84f5-45d4-92e8-4c1042497ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a2bc333e8b4a7ebab5f4081f2ddb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.61 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "def tokenize(batch, tokenizer):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_train_ds = train_ds.map(lambda x: tokenize(x, distilbert_tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c50cb80e-0bb7-42c3-aa16-8b9ad66c2355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e7de44cad74409b47d65f0e15f9d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34e63f3b7e044c0817a5b325237c9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba2382ae4764ba4b9f2749e9d5d1626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f744bd56a274bf8a0cef6074a47aab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "def tokenize(batch, tokenizer):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_train_ds = train_ds.map(lambda x: tokenize(x, distilbert_tokenizer), batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b52a4-24ed-4c01-a4bd-b34e79a2b76f",
   "metadata": {},
   "source": [
    "We are using: truncation to max model input length and padding to max sequence in batch. More on padding and truncation can be found here: https://huggingface.co/docs/transformers/pad_truncation.\n",
    "\n",
    "The tokenization process here is expensive. Therefore, using a higher number of cores to process improved the overall time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c77edf7-dc06-4294-ac28-917dfa119124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', '[SEP]')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = distilbert_tokenizer.convert_ids_to_tokens(tokenized_train_ds[\"input_ids\"][100])\n",
    "tokens[0], tokens[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c75b2-536b-415f-9ce2-3ea7b4dc2b5a",
   "metadata": {},
   "source": [
    "First, some special [CLS] and [SEP] tokens have been added to the start and end of the sequence. These tokens differ from model to model, but their main role is to indicate the start and end of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "941f95c4-d533-4450-933f-f9db1d1c2aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'won', '##t', 'let']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb078cce-1caf-4b48-803f-a28f2024cc5c",
   "metadata": {},
   "source": [
    "The ## prefix in ##t means that the preceding string is not whitespace. We can convert the tokens to a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15adf403-d9cd-4591-80b1-ab93b762606a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i wont let me child cry it out because i feel that loving her and lily when she was little was going to be opportunities that only lasted for those short few months [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd77e9-8352-4239-ba2f-d8d010d6d06b",
   "metadata": {},
   "source": [
    "Also, when a token is not found in the vocabulary the tokenizer can add an unknown token to represent it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd05f3a1-b620-4acb-b1a9-9d0a3905bbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b363e-b6e9-4415-affb-6b575f34cb40",
   "metadata": {},
   "source": [
    "For each batch, the input sequences are padded to the maximum sequence length in the batch; the attention mask is used in the model to ignore the padded areas of\n",
    "the input tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83c868-ac8f-42fc-bf22-983e94d251c8",
   "metadata": {},
   "source": [
    "<center><img src=\"figures/attention_masks.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78b57429-5897-4248-b6e8-f178fca339a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 1, 1, 1, 1, 1, 0], [101, 1045, 2134, 2102, 2514, 26608, 102, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds[\"attention_mask\"][0][:8], tokenized_train_ds[\"input_ids\"][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76eb4ce7-52b7-4685-932c-4c8f2bd8244b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d83aa-5176-488f-b9ea-6e5034690fbe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5a304-15b7-4282-a2be-80dea1cfb2ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_huggingface",
   "language": "python",
   "name": "venv_huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
